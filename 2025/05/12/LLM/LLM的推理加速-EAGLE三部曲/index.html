<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/hexo_blog/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/hexo_blog/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/hexo_blog/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/hexo_blog/images/logo.svg" color="#222">

<link rel="stylesheet" href="/hexo_blog/css/main.css">


<link rel="stylesheet" href="/hexo_blog/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"wnma3mz.github.io","root":"/hexo_blog/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="从标准的自回归解码开始，介绍 EAGLE 系列（EAGLE、EAGLE-2和EAGLE-3）的演进历程。 GitHub">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM的推理加速-EAGLE三部曲">
<meta property="og:url" content="https://wnma3mz.github.io/2025/05/12/LLM/LLM%E7%9A%84%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F-EAGLE%E4%B8%89%E9%83%A8%E6%9B%B2/index.html">
<meta property="og:site_name" content="Wnma&#39;s Blogs">
<meta property="og:description" content="从标准的自回归解码开始，介绍 EAGLE 系列（EAGLE、EAGLE-2和EAGLE-3）的演进历程。 GitHub">
<meta property="og:locale">
<meta property="article:published_time" content="2025-05-12T06:23:00.000Z">
<meta property="article:modified_time" content="2025-08-30T08:45:30.904Z">
<meta property="article:author" content="wnma3mz">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="Attention">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://wnma3mz.github.io/2025/05/12/LLM/LLM%E7%9A%84%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F-EAGLE%E4%B8%89%E9%83%A8%E6%9B%B2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-Hans'
  };
</script>

  <title>LLM的推理加速-EAGLE三部曲 | Wnma's Blogs</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="تشغيل شريط التصفح">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/hexo_blog/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Wnma's Blogs</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/hexo_blog/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/hexo_blog/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/hexo_blog/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/hexo_blog/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-bookmark">

    <a href="https://wnma3mz.github.io/bookmark/index.html" rel="section"><i class="fa fa-bookmark fa-fw"></i>书签</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="https://wnma3mz.github.io/2025/05/12/LLM/LLM%E7%9A%84%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F-EAGLE%E4%B8%89%E9%83%A8%E6%9B%B2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://avatars.githubusercontent.com/u/23001152?s=460&u=dacc012cd237ac86458d888b3723d1d495cb1aa4&v=4">
      <meta itemprop="name" content="wnma3mz">
      <meta itemprop="description" content="Day Day Up">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Wnma's Blogs">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          LLM的推理加速-EAGLE三部曲
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建于：2025-05-12 14:23:00" itemprop="dateCreated datePublished" datetime="2025-05-12T14:23:00+08:00">2025-05-12</time>

            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="عُدل：2025-08-30 16:45:30" itemprop="dateModified" datetime="2025-08-30T16:45:30+08:00">2025-08-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/hexo_blog/categories/Note/" itemprop="url" rel="index"><span itemprop="name">Note</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>从标准的自回归解码开始，介绍 EAGLE
系列（EAGLE、EAGLE-2和EAGLE-3）的演进历程。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/SafeAILab/EAGLE/tree/main">GitHub</a></p>
<span id="more"></span>
<h2 id="llm-的标准生成过程">LLM 的标准生成过程</h2>
<p><img
src="https://raw.githubusercontent.com/wnma3mz/blog_posts/master/imgs/LLM的推理加速-EAGLE三部曲/image.png" /></p>
<p>瓶颈：<strong>token by token</strong> 生成，生成单个 token
的时间很快，但由于要生成<strong>多个</strong>
token，导致整个句子的生成时间变久。</p>
<p>如果能减少生成 token
的<strong>次数</strong>，那么就能加速句子的生成。</p>
<p>一种代表性的优化方法是<strong>投机解码</strong>。</p>
<p>相关引用：<a
target="_blank" rel="noopener" href="https://huggingface.co/blog/tngtech/llm-performance-prefill-decode-concurrent-requests">https://huggingface.co/blog/tngtech/llm-performance-prefill-decode-concurrent-requests</a></p>
<h2 id="投机解码">投机解码</h2>
<p>见：<a href="/hexo_blog/2025/08/30/LLM/LLM%E7%9A%84%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F-%E6%8A%95%E6%9C%BA%E8%A7%A3%E7%A0%81/" title="LLM的推理加速-投机解码">LLM的推理加速-投机解码</a></p>
<h2 id="eagle-1特征层面的投机解码">EAGLE-1：特征层面的投机解码</h2>
<p><a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/2401.15077">https://arxiv.org/pdf/2401.15077</a></p>
<h3 id="eagle-1-的动机">EAGLE-1 的动机</h3>
<p><strong>特征（feature）</strong> 是指下图中的 Output Token
Embeddings，即过完 Decoder-Only Transformer 后的输出</p>
<p><img
src="https://raw.githubusercontent.com/wnma3mz/blog_posts/master/imgs/LLM的推理加速-EAGLE三部曲/image2.png" /></p>
<p>动机一：<strong>采样过程中的不确定性</strong></p>
<p>下图中的 <span class="math inline">\(f_I\)</span>
就是上面说的特征，这个特征会再过 LM Head，得到 logits。这个 logtis
再采样生成 token。</p>
<p>I 后面为是 always 和 am 的概率差不多，如果用 token
级别的采样，可能会漏掉被接受的 token。</p>
<p><img
src="https://raw.githubusercontent.com/wnma3mz/blog_posts/master/imgs/LLM的推理加速-EAGLE三部曲/image3.png" /></p>
<p>动机二： <strong>feature 级别的自回归是比 token
级别的自回归更容易预测</strong></p>
<p>作者是先进行了该项假设，再实现了 EAGLE-1，最后再画了下面这张图。</p>
<ul>
<li>feature&amp;shifted-token：使用特征序列和前一个 token
序列（embedding）进行作为输入</li>
</ul>
<p><img
src="https://raw.githubusercontent.com/wnma3mz/blog_posts/master/imgs/LLM的推理加速-EAGLE三部曲/image4.png" /></p>
<h3 id="prefill-阶段">prefill 阶段</h3>
<p>额外保存变量 + 输出第一个 token</p>
<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr>
<th>示例图</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><img
src="https://raw.githubusercontent.com/wnma3mz/blog_posts/master/imgs/LLM的推理加速-EAGLE三部曲/image5.png" /></td>
<td>从下往上看输入<br>「how」和「can」 两个 token <br><span
class="math inline">\(e_{how}\)</span> 表示「how」这个 token 过完
Embedding 之后的 tensor <br> <span
class="math inline">\(f_{how}\)</span> 表示 「how」这个 token 进入 LM
Head 之前的 tensor<br>预测的 token 为「I」</td>
</tr>
</tbody>
</table>
<h3 id="decode-阶段输出-token">decode 阶段输出 token</h3>
<p>回顾</p>
<ul>
<li>输入的是「how」和「can」，prefill 阶段输出的是「I」</li>
<li>现在的目标是预测「I」的下一个 token</li>
</ul>
<p>EAGLE 的想法：用训练后的 One Auto-regression Head 代替 <strong>N x
transformer layers</strong>。</p>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th>标准解码</th>
<th>EAGLE</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><img
src="https://raw.githubusercontent.com/wnma3mz/blog_posts/master/imgs/LLM的推理加速-EAGLE三部曲/image6.png" /></td>
<td><img
src="https://raw.githubusercontent.com/wnma3mz/blog_posts/master/imgs/LLM的推理加速-EAGLE三部曲/image7.png" /></td>
<td>Step 1:在 prefill 阶段得到前两个 token 的
feature（黄色）以及最新两个 token 的
embedding（绿色）。把它们组合<br>Step 2: 输入到训练后的 One
Auto-regression Head 中<br>Step 3: 输出得到新的特征 <span
class="math inline">\(f_I\)</span> <br>Step 4: 经过 LM
Head，采样得到下一个 token，「make」</td>
</tr>
</tbody>
</table>
<p>FC 的作用是降维，将 feature 和 embedding 两个 tensor 降维成一个。</p>
<ul>
<li>feature：[2, hidden_size]</li>
<li>embedding：[2, hidden_size]</li>
<li>组合 feature 和 embedding：[2, 2*hidden_size]</li>
<li>FC 的权重：[2*hidden_size, hidden_size]</li>
<li>FC 的输出：[2, hidden_size]</li>
</ul>
<h3 id="draft-modelone-auto-regression-head">Draft Model——One
Auto-regression Head</h3>
<h4 id="pipeline">Pipeline</h4>
<ul>
<li>在 Forward 1 中，生成的 f_I 过完 LM Head
后可以采样<strong>多个</strong> token</li>
<li>在 Forward 2 中，用上一轮生成的 token，make 和 help
分别进行下一次循环生成
<ul>
<li>此时，只需要上一次采样生成的 token 信息（Draft Model 信息），而无需
Target Model 的信息</li>
</ul></li>
<li>在 Forward 3 中，同 Forward 2，这里只展示用 with 和 you
进行推理，实际上 a 和 our 也会继续生成</li>
<li>Draft Model （One Auto-regression Head）参数量小，生成 token
的速度<strong>很快</strong>。重复这个步骤</li>
</ul>
<p><img
src="https://raw.githubusercontent.com/wnma3mz/blog_posts/master/imgs/LLM的推理加速-EAGLE三部曲/image8.png" /></p>
<h4 id="draft-tree-每次生成多组可能">Draft Tree 每次生成多组可能</h4>
<p><img
src="https://raw.githubusercontent.com/wnma3mz/blog_posts/master/imgs/LLM的推理加速-EAGLE三部曲/image26.png" /></p>
<p>在一次 Draft Model 推理时，会生成 (Forward 次数)^(采样 token 个数)
的方案</p>
<ul>
<li>make a</li>
<li>make our</li>
<li>help with the</li>
<li>help with your</li>
<li>help you to</li>
<li>help you feel</li>
</ul>
<h4 id="工程设计attention-mask">工程设计：Attention Mask</h4>
<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr>
<th>标准</th>
<th>Draft Tree</th>
</tr>
</thead>
<tbody>
<tr>
<td><img
src="https://raw.githubusercontent.com/wnma3mz/blog_posts/master/imgs/LLM的推理加速-EAGLE三部曲/image9.png" /></td>
<td><img
src="https://raw.githubusercontent.com/wnma3mz/blog_posts/master/imgs/LLM的推理加速-EAGLE三部曲/image10.png" /></td>
</tr>
</tbody>
</table>
<ul>
<li>以「we」为例，这个 token
只能看到「How」和「can」，不能看到「are」</li>
</ul>
<h3 id="对比经典投机采样">对比经典投机采样</h3>
<p>回顾投机采样的收益</p>
<ul>
<li>Draft Model 生成的速度 → 模型的参数量</li>
<li>Draft Model 的<strong>平均接受长度</strong></li>
</ul>
<p>以 72B（Target Model）和 7B（Draft Model）为例</p>
<table>
<colgroup>
<col style="width: 6%" />
<col style="width: 49%" />
<col style="width: 44%" />
</colgroup>
<thead>
<tr>
<th></th>
<th><strong>生成速度</strong></th>
<th><strong>平均接受长度</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>经典方法</td>
<td>7B 的生成速度</td>
<td>只有一组方案，取决于 7B 训练效果</td>
</tr>
<tr>
<td>EAGLE</td>
<td><a
target="_blank" rel="noopener" href="https://docs.vllm.ai/en/latest/features/spec_decode.html?h=speculative#speculating-using-eagle-based-draft-models">0.99B</a>
= 一层 Decoder + FC</td>
<td>生成多组方案，并且是基于 Target Model 训练的</td>
</tr>
<tr>
<td>小结</td>
<td>参数量上远小于 7B</td>
<td>可能性更多，接受率更大经过训练，预期效果会比经典方法好</td>
</tr>
</tbody>
</table>
<p>注：Decoder Layer 的 hidden size 大小于 Target Model 相同</p>
<p>这篇论文实验结果展现的方式不是特别好，没有对比经典投机解码的速度。</p>
<p>在后文中再对比「平均接受长度」，以及其他投机解码方法。</p>
<p>官方结果</p>
<ul>
<li>3x faster than vanilla decoding (Target Model 13B).</li>
<li>投机解码的变种方法
<ul>
<li>2x faster than Lookahead (13B).</li>
<li>1.6x faster than Medusa (13B).</li>
</ul></li>
</ul>
<p><img
src="https://raw.githubusercontent.com/wnma3mz/blog_posts/master/imgs/LLM的推理加速-EAGLE三部曲/demosmall.gif" /></p>
<h2 id="eagle-2工程优化动态草稿树">EAGLE-2：工程优化——动态草稿树</h2>
<p><a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/2406.16858">https://arxiv.org/pdf/2406.16858</a></p>
<p>个人评价：在 Draft Model 生成时用 Beam Search，始终选分数最高的 N
组方案。</p>
<h3 id="eagle-2-的动机">EAGLE-2 的动机</h3>
<h4 id="beam-search">Beam Search</h4>
<p><img
src="https://raw.githubusercontent.com/wnma3mz/blog_posts/master/imgs/LLM的推理加速-EAGLE三部曲/image11.png" /></p>
<ol type="1">
<li>Pos 1：选择 <strong>K
个概率最高的词元</strong>（通常是词语或字符）。这里的 K 是设定的
<strong>束宽 (Beam Width)</strong>。</li>
<li>Pos 2：从两组概率中，结合第一个位置的概率，选出 <strong>K
个最佳的“两词元”序列</strong>。并且是从所有可能的组合中，根据它们的<strong>累积概率</strong>进行排序。</li>
<li>Pos 3：重复上面过程</li>
</ol>
<p>在 EAGLE-1 中提到，Draft Model 每次会生成若干组方案（称为
<strong>Draft Tree</strong>）。</p>
<p>而“若干”是被限定的，如下图所示，每次只采样 2 个 token。</p>
<p><img
src="https://raw.githubusercontent.com/wnma3mz/blog_posts/master/imgs/LLM的推理加速-EAGLE三部曲/image12.png" /></p>
<p>期望更精准地确定 Draft Model 生成的哪些 token
是可靠的（剪枝），<strong>降低 Target Model 验证成本</strong>。</p>
<p>举个例子</p>
<p><img
src="https://raw.githubusercontent.com/wnma3mz/blog_posts/master/imgs/LLM的推理加速-EAGLE三部曲/image13.png" /></p>
<p>限定最大采样 token 数为 2，在生成第二个 token 时，</p>
<ul>
<li>在 EAGLE-1 中，「10+2=」会采样两个 token，1 和 3</li>
<li>在 EAGLE-2 中，「10+2=」只会采样一个 token，1。进而下一个 token 生成
2。</li>
</ul>
<h3 id="验证观察现象">验证/观察现象</h3>
<p><img
src="https://raw.githubusercontent.com/wnma3mz/blog_posts/master/imgs/LLM的推理加速-EAGLE三部曲/image14.png" /></p>
<p>在 <strong>Alpaca 数据集</strong>上，使用 <strong>Vicuna 7B
模型</strong>作为 Target Model，测试 Draft Tree 上不同位置的 Token
接受率。</p>
<ul>
<li>横坐标：不同位置，对应左图 P1-P6。</li>
<li>纵坐标：每个位置的 Token 接受率</li>
</ul>
<p>小结：</p>
<ul>
<li>位置依赖性：P1 （左上角）的接受率相对较高，P6（右下角）
的接受率相对低</li>
<li>上下文依赖：哪怕都在一个位置，接受率存在显著差异。
<ul>
<li>每次 Target Model 校验完，Draft Model 需要重新生成 Draft Tree</li>
</ul></li>
</ul>
<p>为了低成本估计 Draft Token 的接受率，探究了 Draft Model
的置信度分数（即模型输出 token 的概率）与实际接受率之间的关系</p>
<p><img
src="https://raw.githubusercontent.com/wnma3mz/blog_posts/master/imgs/LLM的推理加速-EAGLE三部曲/image15.png" /></p>
<p>同样是在 <strong>Alpaca
数据集</strong>上进行了实验（模型大概也是同一个，<strong>Vicuna
7B</strong>）</p>
<p>如图所示，置信度分数和接受率是<strong>强正相关的。</strong></p>
<ul>
<li>置信度分数低于 0.05 的草稿 token 的接受率约为 0.04</li>
<li>置信度分数高于 0.95 的 token 的接受率约为 0.98</li>
</ul>
<h4 id="置信度具体计算">置信度具体计算</h4>
<p>在过完 Model 后得到 hidden_states，再过 LM head，此时得到的 logits
。最后进行 softmax ，得到每个 token 的置信度。</p>
<p><img
src="https://raw.githubusercontent.com/wnma3mz/blog_posts/master/imgs/LLM的推理加速-EAGLE三部曲/image16.png" /></p>
<h3 id="方法">方法</h3>
<p>定义两个整型变量 K 和 M</p>
<ul>
<li>K 决定每次采样几个 Token（同 EAGLE-1）</li>
<li>M 决定最后生成的 Token 总数</li>
</ul>
<p>假设 K = 2，M = 8</p>
<p>Step 1：每次都生成 2 个 Token，保留每次生成 Token
的概率值。直到所有步骤生成的 Token 总数 &gt; 8</p>
<p>Step 2：每个 Token 的概率值 = 生成该 Token 的概率值 *
父节点的价值。比如 good = It is a good → 1×0.6×0.8×0.7 = 0.34。</p>
<p><img
src="https://raw.githubusercontent.com/wnma3mz/blog_posts/master/imgs/LLM的推理加速-EAGLE三部曲/image17.png" /></p>
<p>Step 3：排序所有 Token 的概率值，选出最大 M=8
个。剩下的不继续向下扩展</p>
<p><img
src="https://raw.githubusercontent.com/wnma3mz/blog_posts/master/imgs/LLM的推理加速-EAGLE三部曲/image18.png" /></p>
<h3 id="比-eagle-1-快在哪">比 EAGLE-1 快在哪？</h3>
<p>Draft Model 生成 token 的<strong>平均接受长度</strong> τ</p>
<p>V 表示 Vicuna
模型，在不同数据集，不同模型尺寸，相较于标准投机解码方法（SpS）的加速比，以及平均接受长度。</p>
<p>最后两列是均值。</p>
<p><img
src="https://raw.githubusercontent.com/wnma3mz/blog_posts/master/imgs/LLM的推理加速-EAGLE三部曲/image19.png" /></p>
<p>官方结果</p>
<ul>
<li><strong>4x</strong> faster than vanilla decoding (13B).</li>
<li><strong>1.4x</strong> faster than EAGLE-1 (13B).</li>
</ul>
<p><img
src="https://raw.githubusercontent.com/wnma3mz/blog_posts/master/imgs/LLM的推理加速-EAGLE三部曲/eagle2.gif" /></p>
<h2 id="eagle-3多层特征融合">EAGLE-3：多层特征融合</h2>
<p><a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.01840">https://arxiv.org/pdf/2503.01840</a></p>
<p>个人评价：feature 雕花 + <strong>训推一致</strong> +
让模型学更难的东西（预测 token vs. 预测 feature）。</p>
<h3 id="eagle-3-的动机">EAGLE-3 的动机</h3>
<p>动机一：基于 EAGLE-2，发现增加训练数据量并不能增加 Draft Model
的平均接受长度。</p>
<p>期望<strong>平均接受长度能随着训练数据量的增加而提高</strong>。</p>
<p><img
src="https://raw.githubusercontent.com/wnma3mz/blog_posts/master/imgs/LLM的推理加速-EAGLE三部曲/image20.png" /></p>
<ul>
<li>横坐标是相对于 ShareGPT 训练数据量，1x、2x、4x、8x
<ul>
<li>ShareGPT 是一个数据集</li>
</ul></li>
</ul>
<p>动机二：特征误差会不断累积</p>
<p>EAGLE 的本质是想用 Draft Model 的输出特征来<strong>近似</strong>
Target Model 的输出特征。</p>
<p>而随着 Draft Model 生成的 token
越多，那么这个近似就会越来越不准（特征误差累积）。</p>
<p>（每次 Target Model 校验后，这个误差会归零）</p>
<ul>
<li><span class="math inline">\(f_t\)</span> 后面会不断增加 <span
class="math inline">\(\hat{f}_{t+1}\)</span> 、 <span
class="math inline">\(\hat{f}_{t+2}\)</span> …</li>
</ul>
<p><img
src="https://raw.githubusercontent.com/wnma3mz/blog_posts/master/imgs/LLM的推理加速-EAGLE三部曲/image21.png" /></p>
<h3 id="推理">推理</h3>
<p>Step 1：输入「How can」，Target Model 生成下一个
token，「I」。同时保存<strong>低层特征（l）、中层特征（m）和高层特征（h）</strong></p>
<p>Step 2：拼接低层 l、中层 m 和高层 h 特征，输入至 <strong>FC</strong>
进行降维，得到<strong>融合特征 g</strong></p>
<p>Step 3：拼接 Step 2 的融合特征 g 和 embedding 信息，再过 FC + Decoder
Layer（EAGLE-1 中的 One Auto-regression Head）</p>
<p>Step 4：采样生成下一个 token「do」，再生成下一个 token
时，会把之前全部信息保留以进行下一次预测。</p>
<p><img
src="https://raw.githubusercontent.com/wnma3mz/blog_posts/master/imgs/LLM的推理加速-EAGLE三部曲/image22.png" /></p>
<p>Q：为什么要保留 <span class="math inline">\(g_{how}\)</span> 、 <span
class="math inline">\(g_{can}\)</span> 作为下一次输入，kv cache
已经有了？</p>
<p>A：这里仅做示意，实际代码中无需再输入</p>
<h3 id="eagle-1-vs-eagle-3">EAGLE-1 vs EAGLE-3</h3>
<h4 id="架构和输入">架构和输入</h4>
<table>
<colgroup>
<col style="width: 8%" />
<col style="width: 43%" />
<col style="width: 48%" />
</colgroup>
<thead>
<tr>
<th></th>
<th><strong>EAGLE-1</strong></th>
<th><strong>EAGLE-3</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>架构</td>
<td>FC-1：[2*hidden_size, hidden_size]Single Decoder Layer</td>
<td>FC-0：[3*hidden_size, hidden_size]FC-1：[2*hidden_size,
hidden_size]Single Decoder Layer</td>
</tr>
<tr>
<td>FC-1 的输入</td>
<td>Target Model 生成的 Feature 和 Embedding</td>
<td>FC-0 生成的 Feature 和 Embedding</td>
</tr>
</tbody>
</table>
<p>从架构来看，EAGLE-3 中间占用了更多显存（影响并发上限）</p>
<h4 id="训练对比">训练对比</h4>
<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr>
<th><strong>EAGLE-1</strong></th>
<th><strong>EAGLE-3</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><img
src="https://raw.githubusercontent.com/wnma3mz/blog_posts/master/imgs/LLM的推理加速-EAGLE三部曲/image23.png" /></td>
<td><img
src="https://raw.githubusercontent.com/wnma3mz/blog_posts/master/imgs/LLM的推理加速-EAGLE三部曲/image24.png" /></td>
</tr>
</tbody>
</table>
<ul>
<li>在 EAGLE-1 中，这里的 <span
class="math inline">\(\hat{f}_{t+1}\)</span> 会随着 Step
的增加而越来越偏（误差累积）</li>
<li>而 EAGLE-3 中，移除了这个 feature 的
loss，并且用<strong>多步生成</strong>的方式训练
<ul>
<li>Draft Model 在第 t 步做出预测 <span
class="math inline">\(\hat{a}_t\)</span> 并采样得到令牌 <span
class="math inline">\(token_t\)</span>
后，在训练下一步时，它会<strong>将自身的预测</strong> <span
class="math inline">\(\hat{a}_t\)</span> <strong>和采样得到的 <span
class="math inline">\(token_t\)</span> 的嵌入作为输入</strong></li>
<li>EAGLE-1 会依赖 Target Model 真实的中间特征</li>
</ul></li>
</ul>
<h4 id="损失函数">损失函数</h4>
<ul>
<li>EAGLE-1：特征预测损失 <span class="math inline">\(l_{fea}\)</span> +
token 预测损失 <span class="math inline">\(l_{token}\)</span> = <span
class="math inline">\(l_{fea}\)</span> + <span
class="math inline">\(l_{token}\)</span> * 0.1</li>
<li>EAGLE-3：token 预测损失 <span
class="math inline">\(l_{token}\)</span></li>
</ul>
<p>具体来说</p>
<ul>
<li><strong>特征预测损失</strong> <span
class="math inline">\(l_{fea}\)</span>
：将下一特征的预测视为回归任务，使用 Smooth L1 损失</li>
<li><strong>token 预测损失</strong> <span
class="math inline">\(l_{token}\)</span> ：基于特征预测通过 LM 头计算
token 分布，使用交叉熵损失</li>
</ul>
<h3 id="比-eagle-2-快在哪">比 EAGLE-2 快在哪？</h3>
<p>在训练阶段，加大了训练数据。最终提升 Draft Model
的平均接受长度<em>（尽管牺牲了生成 Token 的速度）</em></p>
<p>模型</p>
<ul>
<li>V represents Vicuna</li>
<li>L31 represents LLaMA-Instruct 3.1</li>
<li>L33 represents LLaMA-Instruct 3.3</li>
<li>DSL represents DeepSeek-R1-Distill-LLaMA.</li>
</ul>
<p>方法</p>
<ul>
<li>SpS 标准投机解码，Draft Model 是 Vicuna-68M</li>
</ul>
<p><img
src="https://raw.githubusercontent.com/wnma3mz/blog_posts/master/imgs/LLM的推理加速-EAGLE三部曲/image25.png" /></p>
<p>官方结果</p>
<ul>
<li><strong>5.6x</strong> faster than vanilla decoding (13B).</li>
<li><strong>1.8x</strong> faster than EAGLE-1 (13B).</li>
</ul>
<p><img
src="https://raw.githubusercontent.com/wnma3mz/blog_posts/master/imgs/LLM的推理加速-EAGLE三部曲/e3.gif" /></p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>wnma3mz
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://wnma3mz.github.io/2025/05/12/LLM/LLM%E7%9A%84%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F-EAGLE%E4%B8%89%E9%83%A8%E6%9B%B2/" title="LLM的推理加速-EAGLE三部曲">https://wnma3mz.github.io/2025/05/12/LLM/LLM的推理加速-EAGLE三部曲/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-Hans" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/hexo_blog/tags/NLP/" rel="tag"># NLP</a>
              <a href="/hexo_blog/tags/Attention/" rel="tag"># Attention</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/hexo_blog/2025/01/25/Token-Level%E7%9A%84KVCache%E5%A4%8D%E7%94%A8/" rel="prev" title="Token-Level 的 KVCache 复用">
      <i class="fa fa-chevron-left"></i> Token-Level 的 KVCache 复用
    </a></div>
      <div class="post-nav-item">
    <a href="/hexo_blog/2025/08/04/LLM/VLM%E8%AF%86%E5%88%AB%E5%9B%BE%E7%89%87%E8%83%BD%E5%8A%9B/" rel="next" title="VLM的识别图片能力">
      VLM的识别图片能力 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#llm-%E7%9A%84%E6%A0%87%E5%87%86%E7%94%9F%E6%88%90%E8%BF%87%E7%A8%8B"><span class="nav-text">LLM 的标准生成过程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8A%95%E6%9C%BA%E8%A7%A3%E7%A0%81"><span class="nav-text">投机解码</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#eagle-1%E7%89%B9%E5%BE%81%E5%B1%82%E9%9D%A2%E7%9A%84%E6%8A%95%E6%9C%BA%E8%A7%A3%E7%A0%81"><span class="nav-text">EAGLE-1：特征层面的投机解码</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#eagle-1-%E7%9A%84%E5%8A%A8%E6%9C%BA"><span class="nav-text">EAGLE-1 的动机</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#prefill-%E9%98%B6%E6%AE%B5"><span class="nav-text">prefill 阶段</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#decode-%E9%98%B6%E6%AE%B5%E8%BE%93%E5%87%BA-token"><span class="nav-text">decode 阶段输出 token</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#draft-modelone-auto-regression-head"><span class="nav-text">Draft Model——One
Auto-regression Head</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#pipeline"><span class="nav-text">Pipeline</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#draft-tree-%E6%AF%8F%E6%AC%A1%E7%94%9F%E6%88%90%E5%A4%9A%E7%BB%84%E5%8F%AF%E8%83%BD"><span class="nav-text">Draft Tree 每次生成多组可能</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B7%A5%E7%A8%8B%E8%AE%BE%E8%AE%A1attention-mask"><span class="nav-text">工程设计：Attention Mask</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%B9%E6%AF%94%E7%BB%8F%E5%85%B8%E6%8A%95%E6%9C%BA%E9%87%87%E6%A0%B7"><span class="nav-text">对比经典投机采样</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#eagle-2%E5%B7%A5%E7%A8%8B%E4%BC%98%E5%8C%96%E5%8A%A8%E6%80%81%E8%8D%89%E7%A8%BF%E6%A0%91"><span class="nav-text">EAGLE-2：工程优化——动态草稿树</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#eagle-2-%E7%9A%84%E5%8A%A8%E6%9C%BA"><span class="nav-text">EAGLE-2 的动机</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#beam-search"><span class="nav-text">Beam Search</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%AA%8C%E8%AF%81%E8%A7%82%E5%AF%9F%E7%8E%B0%E8%B1%A1"><span class="nav-text">验证&#x2F;观察现象</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BD%AE%E4%BF%A1%E5%BA%A6%E5%85%B7%E4%BD%93%E8%AE%A1%E7%AE%97"><span class="nav-text">置信度具体计算</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%95"><span class="nav-text">方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AF%94-eagle-1-%E5%BF%AB%E5%9C%A8%E5%93%AA"><span class="nav-text">比 EAGLE-1 快在哪？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#eagle-3%E5%A4%9A%E5%B1%82%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88"><span class="nav-text">EAGLE-3：多层特征融合</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#eagle-3-%E7%9A%84%E5%8A%A8%E6%9C%BA"><span class="nav-text">EAGLE-3 的动机</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8E%A8%E7%90%86"><span class="nav-text">推理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#eagle-1-vs-eagle-3"><span class="nav-text">EAGLE-1 vs EAGLE-3</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9E%B6%E6%9E%84%E5%92%8C%E8%BE%93%E5%85%A5"><span class="nav-text">架构和输入</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E5%AF%B9%E6%AF%94"><span class="nav-text">训练对比</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-text">损失函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AF%94-eagle-2-%E5%BF%AB%E5%9C%A8%E5%93%AA"><span class="nav-text">比 EAGLE-2 快在哪？</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="wnma3mz"
      src="https://avatars.githubusercontent.com/u/23001152?s=460&u=dacc012cd237ac86458d888b3723d1d495cb1aa4&v=4">
  <p class="site-author-name" itemprop="name">wnma3mz</p>
  <div class="site-description" itemprop="description">Day Day Up</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/hexo_blog/archives/">
        
          <span class="site-state-item-count">79</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/hexo_blog/categories/">
          
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/hexo_blog/tags/">
          
        <span class="site-state-item-count">91</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/wnma3mz" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;wnma3mz" rel="noopener" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:wnma3mz@gmail.com" title="E-Mail → mailto:wnma3mz@gmail.com" rel="noopener" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">wnma3mz</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script color='0,0,0' opacity='0.5' zIndex='-1' count='99' src="/hexo_blog/lib/canvas-nest/canvas-nest-nomobile.min.js"></script>
  <script src="/hexo_blog/lib/anime.min.js"></script>
  <script src="/hexo_blog/lib/velocity/velocity.min.js"></script>
  <script src="/hexo_blog/lib/velocity/velocity.ui.min.js"></script>

<script src="/hexo_blog/js/utils.js"></script>

<script src="/hexo_blog/js/motion.js"></script>


<script src="/hexo_blog/js/schemes/muse.js"></script>


<script src="/hexo_blog/js/next-boot.js"></script>




  




  
<script src="/hexo_blog/js/local-search.js"></script>











<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'forest',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
